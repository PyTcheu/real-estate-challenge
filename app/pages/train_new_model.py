import streamlit as st
import pandas as pd
from pathlib import Path

st.set_page_config(page_title="Model metrics", page_icon="ğŸ”§", layout="wide")
st.title("ğŸ”§ Model Training Results")

BASE_DIR = Path(__file__).parent.parent  # go up one level to `app/`
metrics_path = BASE_DIR / "new_model" / "training_metrics.csv"

if not metrics_path.exists():
    st.warning("âš ï¸ No training results found. Run `create_new_model.py` first to generate metrics.")
else:
    metrics_df = pd.read_csv(metrics_path)


    st.subheader("ğŸ“Š Model Comparison")
    st.write("Below you can see how each model performed when trained with different types of information:")

    st.markdown("""
    - **Sales_Subset:** Uses only property details (bedrooms, bathrooms, area, etc.)  
    - **All_Features:** Includes both property and neighborhood details (income, population, location, etc.)
    """)

    # Pivot the table
    pivot = metrics_df.pivot(index="Model", columns="Feature_Set", values=["MAE", "MSE", "R2"])
    st.dataframe(pivot.style.format("{:,.2f}"), use_container_width=True)

    st.markdown("""
    ---
    ### ğŸ’¡ Understanding the Results

    Each model tries to predict **house prices** based on the available data.  
    We tested four different approaches:
    **Gradient Boosting**, **K-Nearest Neighbors (KNR)**, **Random Forest**, and **XGBoost**.

    #### ğŸ  What We Found
    - The **Random Forest (All_Features)** model gave the **most accurate predictions**, explaining about **88% of price variations (RÂ² = 0.8797)**.  
    - **XGBoost** followed closely behind, showing similar consistency.  
    - When using only the **Sales_Subset**, performance dropped across all models â€” showing that **location and demographic data play an important role** in explaining price variation.  
    - Simpler methods like **KNN** struggled to adapt to the complexity of the dataset.

    #### ğŸ“Œ Quick Comparison with Baseline Model")

    - **Previous Baseline (KNR with Sales_Subset only)**  
    - MAE â‰ˆ 102k
    - RÂ² â‰ˆ 0.728  

    - **New Best Model (RandomForest with All_Features)**  
    - MAE â‰ˆ 70k
    - RÂ² â‰ˆ 0.88  

    âœ… This shows that including **location, demographic, and neighborhood features** improves accuracy significantly:  
    - **MAE** dropped by ~32%  
    - **RÂ²** increased from 0.73 to 0.88  

    This means the new model is not only more precise but also better at generalizing to unseen data.
    In other words, the new model can provide **more reliable price estimates** for a wider range of properties.

    #### ğŸ† Best Model
    The **Random Forest using All_Features** is the **best fit overall** â€”  
    it balances accuracy and robustness, achieving **RÂ² â‰ˆ 0.88** with low error rates.  
    This indicates a good generalization to new data without overfitting.

    #### âš™ï¸ About This Test
    These models were trained with:
    - **Default settings** (no hyperparameter tuning)  
    - **No advanced feature engineering or EDA**

    This setup serves as a **baseline**, already achieving strong accuracy with minimal tuning â€”  
    proving that including demographic and location features greatly enhances performance.

    ğŸš€ **Next Steps**
    Future improvements could include:
    - Hyperparameter tuning (e.g., GridSearchCV or Optuna)  
    - Adding engineered features (like price per sqft, age of property, etc.)  
    - Testing ensemble methods or stacked models to push RÂ² even higher.
    """)
